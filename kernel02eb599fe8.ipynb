{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport cv2\nimport os\nprint(os.listdir(\"../input\"))\n\nnum_px = 64\n\n# Any results you write to the current directory are saved as output.","execution_count":7,"outputs":[{"output_type":"stream","text":"['chest_xray']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Retireving and Preprocessing images from the respective directories\ndef load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        image = cv2.imread(os.path.join(folder,filename))\n        if image is not None:\n            image = image/255.\n            my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n            images.append(my_image)\n    return images","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Shuffling the datasets\ndef shuffle_in_unison(a, b):\n    assert len(a) == len(b)\n    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n    permutation = np.random.permutation(len(a))\n    for old_index, new_index in enumerate(permutation):\n        shuffled_a[new_index] = a[old_index]\n        shuffled_b[new_index] = b[old_index]\n    return shuffled_a, shuffled_b","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving training (pneumonia) images \ntrain_X_pneumonia_list = load_images_from_folder(\"../input/chest_xray/chest_xray/train/PNEUMONIA\")\ntrain_X_pneumonia = np.array(train_X_pneumonia_list)","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\nUse ``skimage.transform.resize`` instead.\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving training (normal) images \ntrain_X_normal_list = load_images_from_folder(\"../input/chest_xray/chest_xray/train/NORMAL\")\ntrain_X_normal = np.array(train_X_normal_list)","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\nUse ``skimage.transform.resize`` instead.\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging training(pneumonia) and training(normal) datasets into a single dataset and then Shuffling and Flattening it\ntrain_X_orig = np.concatenate((train_X_pneumonia, train_X_normal))\ntrain_Y_pneumonia = np.ones((train_X_pneumonia.shape[0],1))\ntrain_Y_normal = np.zeros((train_X_normal.shape[0],1))\ntrain_Y_orig = np.append(train_Y_pneumonia,train_Y_normal)\ntrain_X, train_Y = shuffle_in_unison(train_X_orig, train_Y_orig)\ntrain_X = train_X.reshape(train_X.shape[0], -1).T","execution_count":19,"outputs":[{"output_type":"stream","text":"(5216, 12288, 1)\n(5216,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving test(pneumonia) images \ntest_X_pneumonia_list = load_images_from_folder(\"../input/chest_xray/chest_xray/test/PNEUMONIA\")\ntest_X_pneumonia = np.array(test_X_pneumonia_list)","execution_count":21,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\nUse ``skimage.transform.resize`` instead.\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving test(normal) normal images \ntest_X_normal_list = load_images_from_folder(\"../input/chest_xray/chest_xray/test/NORMAL\")\ntest_X_normal = np.array(test_X_normal_list)","execution_count":22,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\nUse ``skimage.transform.resize`` instead.\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging test(pneumonia) and test(normal) datasets into a single dataset and then Shuffling and Flattening it\ntest_X_orig = np.concatenate((test_X_pneumonia, test_X_normal))\ntest_Y_pneumonia = np.ones((test_X_pneumonia.shape[0],1))\ntest_Y_normal = np.zeros((test_X_normal.shape[0],1))\ntest_Y_orig = np.append(test_Y_pneumonia,test_Y_normal) \ntest_X, test_Y = shuffle_in_unison(test_X_orig, test_Y_orig)\ntest_X = test_X.reshape(test_X.shape[0], -1).T","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    w = np.zeros((dim,1))\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(w.T,X) + b)   # compute activation\n    cost = -1*np.sum(Y*(np.log(A))  + (1-Y)*(np.log(1-A)))/m    # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n               \n    db = np.sum(A - Y)/m \n    dw = np.dot(X,(A - Y).T)/m\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w, b, X, Y)\n        ### END CODE HERE ###\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        ### START CODE HERE ###\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    ### START CODE HERE ### (≈ 1 line of code)\n    A = sigmoid(np.dot(w.T,X) + b)\n    ### END CODE HERE ###\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        ### START CODE HERE ### (≈ 4 lines of code)\n        if A[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n        ### END CODE HERE ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 200, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = model(train_X, train_Y, test_X, test_Y, num_iterations = 500, learning_rate = 0.005, print_cost = True)","execution_count":33,"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693147\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in log\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in multiply\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n  \n","name":"stderr"},{"output_type":"stream","text":"Cost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\ntrain accuracy: 82.70705521472392 %\ntest accuracy: 82.53205128205128 %\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}